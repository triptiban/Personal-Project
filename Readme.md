# ðŸ§± End-to-End Data Pipeline on Kubernetes (MinIO â†’ Postgres â†’ dbt)

This repository implements an end-to-end, containerized data pipeline that:

1. Extracts e-commerce data from a public API (`https://fakestoreapi.com`)
2. Lands raw JSON files into MinIO (S3-compatible object storage)
3. Loads that raw data into Postgres (`raw.*` schema)
4. Transforms it using dbt (staging â†’ intermediate â†’ marts)
5. (Optional) Exports curated tables back to MinIO  
6. (Bonus) Includes Airflow orchestration for local runs

All components run as Kubernetes workloads and are portable across local or cloud clusters.

---

## âš™ï¸ Architecture

Public API â†’ Extractor(Job) â†’ MinIO (raw/date=YYYY-MM-DD/)
â”‚
â–¼
Loader(Job) â†’ Postgres (raw.*)
â”‚
â–¼
dbt(Job): staging â†’ intermediate â†’ marts
â”‚
â””â”€â”€ (optional) Exporter(Job) â†’ MinIO (curated/)
yaml

---

## ðŸ—‚ï¸ Repository Layout

K8s/ # Kubernetes manifests (MinIO, Postgres, Secrets, Jobs)
extractor/ # Python app: API â†’ MinIO (raw partitioned)
loader/ # Python job: MinIO(raw) â†’ Postgres(raw.* tables)
ecommerce_dbt/ # dbt project (staging/intermediate/marts, tests, snapshots)
dbt-runner/ # Docker image + profiles.yml for dbt
exporter/ # Optional: export curated marts back to MinIO
airflow-local/ # Optional: docker-compose Airflow setup
.github/workflows/ci.yml# GitHub Actions pipeline
scripts/ # Helper scripts (e.g., rerun-all.sh)

yaml


---

## Prerequisites

- Docker 24+
- Kubernetes (e.g. Kind, Minikube, or K3D)
- `kubectl` CLI  
- Optional: `helm` 
- No local Python needed â€” everything runs in containers

---

## Quickstart (Local Kubernetes)

### 1ï¸âƒ£ Create a Kind cluster

```bash
kind create cluster --name ecommerce
Build and load Docker images
docker build -t extractor:0.2 ./extractor
docker build -t loader:0.2 ./loader
docker build -t exporter:0.1 ./exporter
docker build -t dbt-runner:0.1 -f dbt-runner/Dockerfile .

kind load docker-image extractor:0.2 --name ecommerce
kind load docker-image loader:0.2 --name ecommerce
kind load docker-image exporter:0.1 --name ecommerce
kind load docker-image dbt-runner:0.1 --name ecommerce

Deployment & Execution


kubectl -n ecommerce apply -f K8s/minio-secret.yaml
kubectl -n ecommerce apply -f K8s/minio-config.yaml
kubectl -n ecommerce apply -f K8s/postgres-secret.yaml

kubectl -n ecommerce apply -f K8s/minio.yaml
kubectl -n ecommerce apply -f K8s/postgres.yaml
kubectl -n ecommerce apply -f K8s/postgres-init.yaml
Run the Data Pipeline (Jobs)
All Jobs use metadata.generateName, so you must use kubectl create (not apply).

Extract â€” API â†’ MinIO /raw/

kubectl -n ecommerce create -f K8s/extractor-config.yaml
kubectl -n ecommerce create -f K8s/extractor-job.yaml
kubectl -n ecommerce logs -l app=extractor -f --since=1h
Load â€” MinIO /raw/ â†’ Postgres raw.*

kubectl -n ecommerce create -f loader/loader-job.yaml
kubectl -n ecommerce logs -l app=loader -f --since=1h

Transform (dbt) â€” staging â†’ intermediate â†’ marts

kubectl -n ecommerce create -f K8s/dbt-job.yaml
kubectl -n ecommerce logs -l app=dbt -f --since=1h
(Optional) Export â€” curated marts â†’ MinIO /curated/

kubectl -n ecommerce create -f K8s/exporter-job.yaml
kubectl -n ecommerce logs -l app=exporter -f --since=1h
Re-running the pipeline
Each Job creates a fresh run (unique name). To re-run cleanly:

kubectl -n ecommerce delete job -l 'app in (extractor,loader,dbt,exporter)' --ignore-not-found
Then recreate the Jobs using the same commands above.

Tear down everything (optional cleanup)

kubectl -n ecommerce delete all -l app=minio
kubectl -n ecommerce delete all -l app=postgres
kubectl -n ecommerce delete job -l 'app in (extractor,loader,dbt,exporter)' --ignore-not-found
Configuration & Secrets
MinIO
Defined in:

K8s/minio-config.yaml

K8s/minio-secret.yaml

Variable	Description
MINIO_ROOT_USER	Root username
MINIO_ROOT_PASSWORD	Root password
MINIO_BUCKET	Default bucket
RAW_PREFIX	Raw data folder (default: raw/)
CURATED_PREFIX	Curated folder (default: curated/)

Access MinIO console:

bash
Copy code
kubectl -n ecommerce port-forward svc/minio 9000:9000 9001:9001
Visit http://localhost:9001 â†’ login with secret credentials.

Postgres
Defined in:

K8s/postgres-secret.yaml

K8s/postgres.yaml

K8s/postgres-init.yaml

Variable	Description
POSTGRES_DB	Database name (warehouse_db)
POSTGRES_USER	User (warehouse)
POSTGRES_PASSWORD	Password (warehouse_pwd)

Port-forward for DBeaver or psql:


kubectl -n ecommerce port-forward svc/postgres 5432:5432
Connect using:

Host: 127.0.0.1
Port: 5432
Database: warehouse_db
User: warehouse
Password: warehouse_pwd
dbt Project (ecommerce_dbt/)
Model Layers
Layer	Purpose
staging	Raw â†’ typed & cleaned
intermediate	Business join logic
marts	Analytical tables (facts & dims)

Key Models
stg_products, stg_users, stg_carts, stg_cart_items

int_order_items, int_customers

dim_product, dim_customer, fct_sales (incremental)

Snapshots: snapshots/users_snapshot.sql, snapshots/products_snapshot.sql

Run dbt locally (optional)
bash
Copy code
docker run --rm -it -p 8080:8080 \
  -e PG_HOST=localhost -e PG_DB=warehouse_db \
  -e PG_USER=warehouse -e PG_PASSWORD=warehouse_pwd \
  dbt-runner:0.1 bash -lc "cd /work/ecommerce_dbt && dbt docs serve --port 8080 --no-browser"
Open http://localhost:8080 to browse dbt docs.

ðŸ§¾ Example Queries
sql
Copy code
-- Top 10 products by revenue
SELECT p.title, SUM(s.line_amount) AS revenue
FROM analytics.fct_sales s
JOIN analytics.dim_product p USING (product_id)
GROUP BY 1
ORDER BY revenue DESC
LIMIT 10;

-- Repeat customers
SELECT c.customer_id, c.full_name, COUNT(DISTINCT s.cart_id) AS orders
FROM analytics.dim_customer c
JOIN analytics.fct_sales s ON s.customer_id = c.customer_id
GROUP BY 1,2
HAVING COUNT(DISTINCT s.cart_id) > 1
ORDER BY orders DESC;
ðŸ§© CI/CD Integration
GitHub Actions: .github/workflows/ci.yml (already present)

GitLab required by assignment â€” add .gitlab-ci.yml:


stages: [build, test, deploy, transform]

build:
  stage: build
  image: docker:24
  services: [docker:24-dind]
  script:
    - docker build -t extractor:$CI_COMMIT_SHORT_SHA ./extractor
    - docker build -t loader:$CI_COMMIT_SHORT_SHA ./loader
    - docker build -t exporter:$CI_COMMIT_SHORT_SHA ./exporter
    - docker build -t dbt-runner:$CI_COMMIT_SHORT_SHA -f dbt-runner/Dockerfile .

test:
  stage: test
  image: dbt-runner:$CI_COMMIT_SHORT_SHA
  script:
    - cd /work/ecommerce_dbt
    - dbt deps
    - dbt build --select state:modified+

deploy:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl apply -f K8s/minio.yaml
    - kubectl apply -f K8s/minio-secret.yaml
    - kubectl apply -f K8s/minio-config.yaml
    - kubectl apply -f K8s/postgres-secret.yaml
    - kubectl apply -f K8s/postgres.yaml
    - kubectl apply -f K8s/postgres-init.yaml

transform:
  stage: transform
  image: bitnami/kubectl:latest
  script:
    - kubectl create -f K8s/extractor-job.yaml
    - kubectl create -f loader/loader-job.yaml
    - kubectl create -f K8s/dbt-job.yaml
    - kubectl create -f K8s/exporter-job.yaml || true
ðŸ©º Observability
readinessProbe and livenessProbe defined for Postgres & MinIO

Logs:

bash

kubectl -n ecommerce logs job/<job-name>
Optional: integrate Prometheus annotations or Grafana dashboards

âœ… Gaps Checklist (whatâ€™s optional / to polish)
Area	Status	Notes
GitLab CI	ðŸ”§ Required	.gitlab-ci.yml provided above
Architecture diagram	ðŸ”§ Optional	Add docs/architecture.png and embed
Airflow orchestration	âœ… Included (docker-compose)	Optional for bonus points
Secrets via env	âœ… Configured properly	
Health checks	âœ… Basic probes present	
dbt tests	âœ… Included	
Snapshots	âœ… Included	
Incremental models	âœ… fct_sales	
Docs generation	âœ… via dbt Job	

Cleanup
bash
Copy code
kind delete cluster --name ecommerce
Author: Tripti Bansal
Tech Stack: Kubernetes â€¢ Python â€¢ MinIO â€¢ PostgreSQL â€¢ dbt â€¢ Docker â€¢ Airflow (optional)